---
title: "RaukR_notes"
author: "CPB"
format: html
editor: visual
code-overflow: scroll
affiliation: PMC
date-modified: date-modified
execute:
  eval: false
toc: true
---

# Scripts, functions, and best practices

To run executable Rscript:

1.  Make script executable `chmod +x myscript.R`
2.  First line of script should be `#!/usr/bin/env Rscript`

### Text streams in R (pipe results to script)

```{r}
input_con  <- file("stdin")
open(input_con)
oneline=readLines(input_con, n = 1)
close(input_con)
```

Tidyverse can read a `tibble` from text stream: `read_csv(file("stdin"))`

Any `stdout`produced by the code (`print()`, `cat()`, etc) can be piped to a new process

To write a `tibble`as a text stream: `cat(format_csv(my_tibble))`

### Hidden variables/functions

Hidden functions - for helper functions that the end user doesn't need to access

Still there but users can't see them. Same for hidden variables.

List hidden items `ls(all.names = TRUE)`

### Functions

Inside function - `a <<- "abc"` will assign the variable outside the function - **don't do**

Good practice to give `data` as first argument, especially when using pipes `%>%`

```{r}
#| eval: false
myfun <- function(x, arg)
```

Set arguments to defaults --- better too many args than too few. Also use elipses `â€¦`

#### Closure

Function inside functions. There's some static part to the environment

Double arrow affects global environment one step up, so doesn't affect the "real" global env. So other functions that use `i` are not affected.

```{r}
new_counter <- function() {
  i <- 0
  function() {
    # do something useful, then ...
    i <<- i + 1
    i
  }
}

counter1 <- new_counter(); counter2 <- new_counter()
counter1(); counter1(); counter2()
```

First call to counter1 is `i=1` , counter2 is `i=2`

#### Wrapper functions

With wrapper functions you can set behavior of existing functions/plots. For example, setting the color, size of text, etc

```{r}
my_awesome_plot <- function(x, ...) {
  plot(x, col='red', pch=19, cex.axis=.7, ...)
}
```

### Check

S4 and R6 classes - during OOP block

# Quarto documents

Set `freeze : true` in the yaml header to "cache" the rendered parts. Only the parts you change will be re-rendered. Avoids running and rendering everything again.

```{yaml}
execute:
  freeze: true
```

Chunk options define how chunks behave

-   `eval: false` to not evaluate a code chunk
-   `echo: false` to hide input code
-   `output: true` to show output, `asis` to skip styling
-   `warning: false` hides warnings
-   `message: false` hides messages
-   `error: true` shows error message and continues code execution
-   `include: false` suppresses all output

## Interactive plots

**plotly** - https://plotly.com/ggplot2/ - interactive ggplots with not too much extra work

**rbokeh** - https://hafen.github.io/rbokeh/articles/rbokeh.html

[Interactive stuff documentation](https://quarto.org/docs/interactive/)

## Projects

Common yaml parameters in `_quarto.yml`

```{yaml}
project:
  output-dir: _output

toc: true
number-sections: true
  
format:
  html:
    css: styles.css
  pdf:
    documentclass: report
    margin-left: 30mm
```

[Project documentation](https://quarto.org/docs/projects/quarto-projects.html)

#### Callouts

Show warnings, notes, tips, important, etc

#### Diagrams

Create diagrams with code.

# Debugging

-   Classes and type checking

-   Loops usually fail at the beginning or at the end - testing at the boundaries

-   Give small dataset to test

-   *antibugging*Â ðŸ•¸:Â `stopifnot(y <= 75)`

## Dumping frames

R state is saved to file, which can be read with a debugger. Allows you to check values of variables

```{r}
options(error = quote(dump.frames(dumpto = "assets/testdump", to.file = T)))
```

## Traceback

Shows what were the function calls and what parameters were passed to them when the error occurred.

## Debug

Using thee `debug` function

```{r}
#| eval: false
h <- function(x, y) { 
  f(x) 
  f(y) 
}

debug(h)
h('text', 7)
undebug(h)
```

## Profiling

### proc.time()

*user time* -- CPU time charged for the execution of user instructions of the calling process,

*system time* -- CPU time charged for execution by the system on behalf of the calling process,

*elapsed time* -- total CPU time elapsed for the currently running R process.

### system.time()

## Optimization

-   vectorize and allocate memory

-   `microbenchmark`

-   Matrix multiplications run faster on GPUs

## Parallelization

-   `parallel`

```{r}
library(parallel)
num_cores <- detectCores() - 1
cl <- makeCluster(num_cores) # Init cluster
parLapply(cl, 1:2, function(x) { c(x, x^2, x^3)} )
stopCluster(cl)
```

# Others

Parenthesis around expression, etc -\> print

```{r}
(vec <- seq(0.1, 0.9, by=0.1))
```

`near` function - safe way of comparing if two vectors of floating point numbers are (pairwise) equal. This is safer than using `==`, because it has a built in tolerance

`simpleWarning` & `simpleErrors`

`ggplot autoplot` - Create a complete ggplot appropriate to a particular data type

# Vectorization

Vectorize user-written functions: `Vectorize` from the base package

```{r}
#| eval: false
vectorized_is_a_droid <- base::Vectorize(is_a_droid, vectorize.args = c('x'))
vectorized_is_a_droid(test)
```

`*apply` takes matrix as input, not vector, need to convert vector to matrix.

`lapply` - apply function to each element of list

`vapply` - you can specify expected output format, error if output not in that format

`purrr` - map function to elements of vector and use `unlist`

# Parallelization

`Future` package

Doesn't block using R during computing. It's possible to keep using R while computing in other cores

> Unblocked R process during resolving of futures process

variable `%<-%` {expression(s)}

-   sequential - use during development

-   multisession -starts different R sessions to parallelize

-   multicore - use different cores for paralleliization

### Sequential

Need to set `plan(sequential)` as first line, like calling `library()`

```{r}
library(future)
plan(sequential)

a %<-% {
  Sys.sleep(3)
  a <- 1
}
b %<-% {
  Sys.sleep(3)
  b <- 2
}

a + b
```

Check number of cores in system `availableCores()`

```{r}
plan(multicore)

a %<-% {
  Sys.sleep(3)
  a <- 1
}
b %<-% {
  Sys.sleep(3)
  b <- 2
}

a + b

```

### Cluster

`plan(cluster)`

```{r}
#| eval: false
plan(cluster, workers = c("n1", "n2", "n3"))
```

Specialized R package for interfacing with common HPC job schedulers exists:Â `future.batchtools`

# Object oriented programming

Methods inside functions == functions

Classes can inherit properties from other classes

### S4 classes

Object name begins with `.` by convention

`sealed =T` prevents redefining the same class (with same name)

Access slots in class with `@`

# TidyR

### New pipes

-   the `%>%` pipe

-   `x %>% f` â‰¡ `f(x)`

-   `x %>% f(y)` â‰¡ `f(x, y)`

-   `x %>% f %>% g %>% h`â‰¡ `h(g(f(x)))`

-   `%T>%` - split data and send to the next two function calls

```{r}
rnorm(50) %>% 
  matrix(ncol = 2) %T>% 
  plot() %>% 
  summary()
```

`%T>%` sends data to both `plot` and `summary`, not `plot` and *then* `summary`

-   `%$%` - for functions that don't take data as the first argument, like cor = `cor(data$1, data$2)`. Exposition of variables.
-   `|>` base R pipe

```{r}
mtcars |> lm(mpg ~ disp, data = _)
```

The `_` placeholder only works for arguments that are named in the function

Sometimes we want to pass the resulting data to *other than the first* argument of the next function in chain. `magritter` provides placeholder mechanism for this:

-   `x %>% f(y, .)` = `f(y, x)`,

-   `x %>% f(y, z = .)` = `f(y, z = x)`.

*\`%\*% (.,.)* - instead of doing x\*y, it will do \*(M,M)

When sending item through pipe, need to use `{}` around the function so that the item only goes to the `.`, so it's not evaluated as the first item in the function

ex: `M %>% {print_M_summ(nrow(.), ncol(.))}`

Functional sequence / sequence of functions: `f <- . %>% sin %>% cos`

## Tibbles

When creating a tibble, a column can be a function of other columns

Change how tibbles are saved

-   `my_tibble %>% print(n = 50, width = Inf)`,
-   `options(tibble.print_min = 15, tibble.print_max = 25)`,
-   `options(dplyr.print_min = Inf)`,
-   `options(tibble.width = Inf)`

#### Parsing

Locale = system definitions, which keyboard, time zone, etc

```{r}
charToRaw(text2)
parse_character(text1, locale = locale(encoding = 'UTF-8'))
guess_encoding(charToRaw("Test"))
guess_encoding(charToRaw(text2))
```

### Data transformations

`arrange` = `order` from data.table

`select(-(x:z))` remove columns x-z

`%>% select(x:z, everything())` reorder columns with x:z in the beginning, and all the rest following

`mutate` create / change variables

`transmute` - only keeps create columns

`group_by` - perform actions per group (columns with grouping)

`n()` count occurrences of category - like data.table `.N`

`pivot_longer` - turn columns into rows

```{r}
bijou2 %>% 
  pivot_longer(cols = c(`2008`, `2009`), names_to = 'year', values_to = 'price') %>% 
  head(n = 5)
```

`pivot_wider` - turn rows into columns

```{r}
bijou3 %>% 
  pivot_wider(names_from = dimension, values_from = measurement) %>% 
  head(n = 5)
```

`unite` - combine columns

`complete` - complete missing values

`left_join` keep all "keys" from first table

`inner_join` only keep common "keys"

`right_join` keep all keys from second table

`full_join` keep all "keys"

eager - needs the data to exist to use it. lazy - the data can be created in the same table creation call

# Git

`git reset HEAD~1` - go back `n` commits. The changes will still be there, we need to `restore` to take it back to how it was before this commit

`git restore <our R script>`

-   `git branch <name of branch>` : Create a new branch.
-   `git branch` : See list of branches. Current branch marked with \*.
-   `git checkout <name of branch>` : Move to branch.
-   `git merge <name of branch>` : Merge the branch you are currently on with the branch named in command.

# Reticulate (R + python)

#### Import python modules to R

```{r}
library(reticulate)
datetime <- import("datetime") # datetime is pythono module
todays_r_date <- datetime$datetime$now()
```

Python function returns R object!

`convert = F` in import statement -\> don't convert to R objecct

#### Access Python's built-in functions directly in R

```{r}
builtins <- import_builtins()
r_vec <- c(1, 5, 3, 4, 2, 2, 3, 2)
str(r_vec)
builtins$len(r_vec)
```

#### Possible to source own python scripts

```{r}
source_python("python_functions.py")
```

#### Execute python code

```{r}
py_run_string("result = [1,2,3]*2")
py$result
```

```{python}
a = len([1,2,3])
```

#### Run python scripts

`py_run_file(filename)`

Access python results with `py$<variable>`

```{r}
py$a
```

`py_to_r` - converts python object to R

# ggplot

ggplot prefers data in long format

`geom_column` - best for histogram?

`help.search("^geom_",package="ggplot2")` search for available geoms

`stat` inside geom call

-   stat="bin"

-   stat="count" - how many elements in that group

-   geom_line(stat= density)

-   identity - calculate sum of y var, grouped by x var

`geom_bar` - counts the occurrences of each unique x

`args(geom_bar)` to check arguments available

aesthetic mapping - inside `aes`

aesthetic parameter - doesn't matter where specified

`scale`

-   `scale_<aesthetic>_<type>`

-   `scale_<axis>_<type>`

`facet_wrap` - tilde specifies that it's creating a matrix. One dimension

`facet_grid` for two variables being compared. Can be two dimensions

`coord_cartesian(xlim=c(2,8))` to zoom

`element_rect` change background of plot

```{r}
element_rect(fill=NULL,color=NULL,size=NULL,linetype=NULL)
p <- p + theme(
    plot.background=element_rect(fill="#b3e2cd"),
    panel.background=element_rect(fill="#fdcdac"),
    panel.border=element_rect(fill=NA,color="#cbd5e8",size=3),
    legend.background=element_rect(fill="#f4cae4"),
    legend.box.background=element_rect(fill="#e6f5c9"),
    strip.background=element_rect(fill="#fff2ae")
)
```

#### Interactive - ggplotly

```{r}
ggplotly(p1)
```

Edit legend - stack items in one label, etc

```{r}
guides(size=guide_legend(nrow=2,byrow=TRUE),
         color=guide_legend(nrow=3,byrow=T))
```

-   `ggrepel` - non-overlapping labels

Add random things to the plot

```{r}
+ annotate("text",x=2.5,y=2.1,label="There is a random line here")+
  annotate("segment",x=2,xend=4,y=1.5,yend=2)
```

```{r}
ggplot(data=iris,mapping=aes(x=Petal.Length,y=Petal.Width))+
  geom_point(aes(color=Species))+
  annotate("text",x=2.5,y=2.1,label="There is a random line here")+
  annotate("segment",x=2,xend=4,y=1.5,yend=2)
```

# PURRR

```{r}
library(gapminder)
```

Combining two vectors can be done with `paste`

```{r}
child <- c("Reed", "Wesley", "Eli", "Toby") 
age <-c( 14, 12, 12, 1)
paste(child, "is", age, "years old")
```

`glue` - package to combine strings and interpolating - similar to `fstrings`?

A lot of loops are not necessary

`purrr` is an alternative to the `apply` functions

`View(<list>)` manually inspect list and automatically create code along the way - click the button on the right

#### map (for loop alternative)

`purrr::map` - takes vector/list of size N and returns object of size N

-   `map(.x, .f)`

dot in front or argument names - reduce name collisions

#### R lambda functions:

AKA anonymous functions

```{r}
map(got_chars, \(x) length(x[["aliases"]]))
map(got_chars, ~ length(.x[["aliases"]]))
```

`\(x) <expression>` - base R

`~ <expression>` - tidyR version

Set names in purrr pipelines early, they'll be available for the rest of the analyses.

```{r}
got_chars_named <- set_names(got_chars, map_chr(got_chars, "name"))
```

`enframe` - first col is names, second is list

`tibbles` designed to be list friendly

#### Set list names for a happier life.

`got_chars_named <- set_names(got_chars, map_chr(got_chars, "name"))`

#### There are many ways to specify .f.

```{r}
map(got_chars, \(x) length(x[["aliases"]]))
map_chr(got_chars, "name")
map(sw_vehicles, list("pilots", 1))
```

#### .default is useful for missing things.

```{r}
map(sw_vehicles, "pilots", .default = NA)
map_chr(sw_vehicles, list("pilots", 1), .default = NA)
```

`discard` function - can take anonymous functions - removes those elements

`walk` - do something for every element in the list, but don't do anything

`map2` - map over two arguments

`pmap` - parallel map

Can transform JSON API call into tibble/df with R

-   `hoist` - choose which cols to keep, rest is in `raw`

-   `unnest_wider` - get one column for every field

```{r}
got_dat <- tibble(raw = got_chars)
got_dat |>
  hoist(raw, "name", "culture", "alive", first_aliases = list("aliases", 1))
```

`rowwise` works as `group_by` where every row is a group

`across` - do something for each column

`str_flatten_comma` - combines vector into string split by `,`

# Packages

Package in "source" form

`DESCRIPTION` - description of package -metadata. Includes dependencies

`R` - contains .r files with function defenitions

`vignettes`

`NAMESPACE`

There should be type/behavior checks in the functions - *assertions*

Steps:

1.  `create_package("~/path/to/<package_name>")`
2.  `library(devtools)`
3.  `use_git()`
4.  `use_github()`
5.  `use_r("<my_function>")` - creates file for function
6.  **`load_all()`** - make created functions available for testing
    1.  `cmd+shift+L` - shortcut for `load_all(".")` - loads functions in the package
7.  `check()` - check that R package is in working order, if it's a proper package
    1.  0 errors, 0 warnings, 0 notes

    2.  run often

    3.  solve issues as soon as they show up
8.  Edit DESCRIPTION
    1.  Author info, title, description

    2.  Add licence - **`use_mit_license()`**
9.  Add function documentation
    1.  `.Rd` file - R documentation

    2.  Go to function file and add comments. These should start with `#'` - place cursor inside function - go to `code` in the toolbar - `Insert Roxygen skeleton` -- inserts skeleton of function documentation

        1.  `Cmd+Alt+Shift+R`

```{r}
#' Split a string
#'
#' @param x A character vector with one element. # what the arguments are
#' @param split What to split on.
#'
#' @return A character vector.
#' @export
#'
#' @examples
#' x <- "alfa,bravo,charlie,delta"
#' strsplit1(x, split = ",")
```

1.  `document()`
    1.  `Cmd+Shift+D`
2.  `check()`
3.  `install()` - installs packages where other R packages are
4.  restart R
5.  `library(<package name>)` - **Functions ready to use!!!**

## Testing

Fix bug in package - create unit test for that specific bug

1.  Initiate testing `use_testthat()`
2.  With the function file open, create companion test file with `use_test()`
3.  Edit inside test file - supposed to read like a sentence, starts wth `test_that` , and expectations `expect_equal`
    1.  `expect_equal` - first thing (function from package) should be equal to the second thing (expected result)
4.  run `load_all`
5.  run tests - either control enter, `test_active_file()`, `Run Tests` button, `test()`

## Use third-party packages in your package

-   `use_package("<package_name>")` - in package code, use functions from other packages as `<package>::<function>()`

-   it's possible to inherit documentation from another package

-   After changes to the functions (renaming, adding parameters, etc)

    -   `load_all()` --\> `document()` -\> `load_all()` --\> check()

Rename files

-   `rename_files("<old_name>", "<new_name>")` - names without extension

    -   automatically renamed several files

### Set automatic check() with github actions

`use_github_action(name = "check-standard")`

## Create README

`use_readme_rmd()` - creates `.rmd` file

`build_readme()`

## Host github pages

`use_pkgdown_github_pages()` - adds github actions configuration,

# Shiny

Needs a live environment

Two file format for complex apps

Server in one file, UI in another file

Access data in input widget `input$`

Access output n `output$`

# Machine learning & statistics

N should be \> P, but in biology often not possible

N = statistical observations (samples, cells)

P = features (genes, proteins, etc)

The curse of dimensionality - if P \>N, in naive statistical analysis ...

can't increase dimentsionality infinitely.

slide 7 - can't detect effect because data is high dimensionall - no diff between cases and controls because of the curse of dimensionality - points become far from each other, and equidistant

#### Regularizations - LASSO

OLS - ordinary least square

Also considered feature selection

Penalized OLS `+Î»(|Î²1|+|Î²2|)` introduce *good* bias - need to prioritize features

Cross validation - split data set into pieces several times - do optimization for lambda in most pieces

With LASSO we start getting close to machine learning

we want to find optimal value of `Î»` , after this we find `Î²1` and `Î²2` - model is trained when you find optimal `c` and `Î²`

slide 8 - y-axis = error, x-axis = `Î»`

minimum = optimal `Î»` ?

With LASSO you do bayesian statistics

`L` = likelihood

## Markov Chain Monte Carlo

Needs a lot of points (expensive)

Markov chain - good way to find the region close to the object we want

without knowing the shape of the distribution, it can find where to sample

markov process - the next step only remembers the previous step, not the other before

a step is accepted if it leads to an increase in the posterior

slide 11 -\
(n1+2n2)/2(n0+n1+n2) - max Likelihood - optimization of likellihood

comes from HW equilibrium\

K-means - mark K points as cluster center - initial center of clusters

check which center a certain point is closest to - then it belongs to that cluster

create centroid at the center of the cluster. Compute k-means again until centers don't move

traditional K-means used euclidean distance (which is not good for high dimentional space)

slide13 - case of missing heritability - lack of prediction. in GWAS you find a lot of genes, but it doesn't mean they are actionable

you can find genes associated with disease, but that doesn't mean you can use them for treatment/prognosis

ML is based on prediction

## Neural network

neural network - you don't know the function, but you end up inferring it

nodes, edges (with weights)

NN with mild assumptions can approximate any function

to avoid over-fitting - use error on test dataset. Looking at how error changes while training.

ex: error not decreasing, accuracy plateauing

low number of samples - over-fitting

core of NN is gradient descent

### Decision tree

Gini index is optimization function. Tries to make classes as pure as possible

gini index = 0 -\> very pure; split with lowest gini index is best split

sometimes better to cut tree prematurely to not over fit - slide 24 - max_depth

### Random forest

Extension from decision tree (which is prone to overfitting) - trains multiple trees. Doesn't use all features in the data, but for each tree it uses a fraction of features. This way trees become uncorrelated. If they're correlated, they're not very informative.

`Error = bias+variance`

The more trees you train, the lower the variance gets - ensemble learning

Train several trees and average the results

if there are correlated features - go for LASSO instead of linear regression

Always start with a simple model, like linear regression --\> random forest ---\> NN

Deep learning is good for big data - huge datasets - ex: single cell

One patient is one statistical observation

Big data in life science - image data, single-cell, metagenomics

# Tidymodels

Many times, clustering can be replaced by basic statistics like ANOVA or t-test

brier score to measure performance. Smaller values are better

Important that the model is well callilbrated

pillar package - how tibbles print

tidymodels_prefer() - use tidy functions

Validation set taken from the training set to validate results during development. Testing set should not be touched before testing

Data from the same patient shouldn't be both in training and in testing

Logistic regression - binary outcome. linear-ish model

engine - the way we fit the model

The `broom` package takes the messy output of built-in functions in R, such asÂ `lm`,Â `nls`, orÂ `t.test`, and turns them into tidy tibbles

# 

#### `workflow()`

A workflow captures the entire modeling process:Â `fit()`Â andÂ `predict()`Â apply to the preprocessing steps in addition to the actual model fit

ensures that you don't have data leakage - you can't use the same dataset for testing and training

dummy variables - convert non-numeric variables to binary variables - create new columns that have `1` if var present, if not, `0`

re-sampling - use training set to get performance

V-fold cross-validation - usually use 10 - then average the results

### Bootstrap

Set with same num of elements as the original set, but items were taken with replacement.

samples that were never included in the bootstrap set will be used to estimate performance

"out of bag data" - not sampled - the leftovers

### Random forest

number of trees should not be a tuning parameter.

It's possible to set how many variables we want to use as predictors (randomly selected)- `mtry` (`rand_forest()`) - parsnip package)

Random forest can deal with missing data

Bagging - lower performance than random forest

Create a random forest model

```{r}
library(tidymodels)
rf_spec <- rand_forest(trees = 1000, mode = "classification")
rf_spec
```

```{r}
rf_wflow <- workflow(class ~ ., rf_spec)
rf_wflow
```

Evaluating model performance

```{r}
ctrl_rs <- control_resamples(save_pred = TRUE)

# Random forest uses random numbers so set the seed first

set.seed(2)
rf_res <- fit_resamples(rf_wflow, cell_rs, control = ctrl_rs, metrics = cls_metrics)
collect_metrics(rf_res)
```

kap = 0 - good

kap \< 0 - do the opposite of what the model tells you

```{r}
collect_predictions(rf_res) %>% 
  ggplot(aes(.pred_PS)) + 
    geom_histogram(col = "white", bins = 30) + 
    facet_wrap(~ class, ncol = 1)
```

Predictors don't work in text data, they need numeric data - convert to binary

Centering and scaling data - required by some models - all columns wit mean of zero, stdeviation = 1
